pip install tflite-model-maker
pip install tensorflow

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from pathlib import Path
import gdown
import zipfile
import os

# Fungsi untuk mengunduh file dari Google Drive
def download_file_from_google_drive(file_id, dest_path):
    url = f"https://drive.google.com/uc?id=1_YY46WkTmBW0ySlq2ifZg2lHuIfI4OOP"
    gdown.download(url, dest_path, quiet=False)

# ID file Google Drive
file_id = 'YOUR_FILE_ID'  # Ganti dengan ID file Google Drive Anda

# Nama file untuk menyimpan dataset
dataset_zip = 'dataset.zip'

# Unduh dataset
download_file_from_google_drive(file_id, dataset_zip)

# Periksa apakah file yang diunduh adalah file ZIP yang valid
if zipfile.is_zipfile(dataset_zip):
    # Ekstrak dataset
    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
        zip_ref.extractall('dataset')
    print("File extracted successfully")
    # Hapus file zip setelah ekstraksi
    os.remove(dataset_zip)
else:
    raise Exception("Downloaded file is not a valid ZIP file")

# Path ke direktori dataset yang diekstrak
dataset_dir = Path('dataset')

# Buat dataset dari direktori
train_dataset = image_dataset_from_directory(
    dataset_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(224, 224),
    batch_size=32)

validation_dataset = image_dataset_from_directory(
    dataset_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(224, 224),
    batch_size=32)

# Debug: Print jumlah batch dan label untuk verifikasi
for images, labels in train_dataset.take(1):
    print(f"Training batch shape: {images.shape}")
    print(f"Training labels: {labels.numpy()}")

for images, labels in validation_dataset.take(1):
    print(f"Validation batch shape: {images.shape}")
    print(f"Validation labels: {labels.numpy()}")

# Normalisasi dataset
normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)
normalized_train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))
normalized_validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))

# Definisikan model dengan MobileNetV2 sebagai basis
base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
                                               include_top=False,
                                               weights='imagenet')

base_model.trainable = False  # Freeze the base model

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(len(train_dataset.class_names), activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Debug: Print summary model
model.summary()

# Latih model
history = model.fit(
    normalized_train_dataset,
    validation_data=normalized_validation_dataset,
    epochs=10
)

# Evaluasi model
loss, accuracy = model.evaluate(normalized_validation_dataset)
print(f'Validation loss: {loss:.2f}')
print(f'Validation accuracy: {accuracy:.2f}')

# Simpan model sebagai model TensorFlow
export_dir = 'saved_model'
tf.saved_model.save(model, export_dir)

# Konversi model ke TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

# Simpan model TensorFlow Lite
with open('barcode_reader_model.tflite', 'wb') as f:
    f.write(tflite_model)

print("Model TFLite saved as barcode_reader_model.tflite")
